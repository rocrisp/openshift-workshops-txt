# Operator Lifecycle Manager

# The Operator Lifecycle Manager (https://github.com/operator-framework/operator-lifecycle-manager) project is a component of the Operator Framework, an open source toolkit to manage Kubernetes native applications, called Operators, in an effective, automated, and scalable way.

# OLM extends Kubernetes to provide a declarative way to install, manage, and upgrade operators and their dependencies in a cluster. It also enforces some constraints on the components it manages in order to ensure a good user experience.

# OLM enables users to do the following:

# # Over-the-Air Updates and Catalogs

# Kubernetes clusters are being kept up to date using elaborate update mechanisms today, more often automatically and in the background. Operators, being cluster extensions, should follow that. OLM has a concept of catalogs from which Operators are available to install and being kept up to date. In this model OLM allows maintainers granular authoring of the update path and gives commercial vendors a flexible publishing mechanism using channels.

# # Dependency Model

# With OLM's packaging format, Operators can express dependencies on the platform and on other Operators. They can rely on OLM to respect these requirements as long as the cluster is up. In this way, OLM's dependency model ensures Operators stay working during their long lifecycle across multiple updates of the platform or other Operators.

# # Discoverability

# OLM advertises installed Operators and their services into the namespaces of tenants. They can discover which managed services are available and which Operator provides them. Administrators can rely on catalog content projected into a cluster, enabling discovery of Operators available to install.

# # Cluster Stability

# Operators must claim ownership of their APIs. OLM will prevent conflicting Operators owning the same APIs being installed, ensuring cluster stability.

# # Declarative UI controls

# Operators can behave like managed service providers. Their user interface on the command line are APIs. For graphical consoles OLM annotates those APIs with descriptors that drive the creation of rich interfaces and forms for users to interact with the Operator in a natural, cloud-like way.The Operator Lifecycle Manager (OLM) is included with [OpenShift 4](https://try.openshift.com) and can easily be installed in a non-OpenShift Kubernetes environment by running [this simple command](https://operatorhub.io/how-to-install-an-operator#).

# Let's get started exploring OLM by viewing its Custom Resource Definitions (CRDs).

# OLM ships with 6 CRDs:

# * **CatalogSource**:
#     * A collection of Operator metadata (ClusterServiceVersions, CRDs, and PackageManifests). OLM uses CatalogSources to build the list of available operators that can be installed from OperatorHub in the OpenShift web console. In OpenShift 4, the web console has added support for managing the out-of-the-box CatalogSources as well as adding your own custom CatalogSources. You can create a custom CatalogSource using the [OLM Operator Registry](https://github.com/operator-framework/operator-registry).
# * **Subscription**:
#     * Relates an operator to a CatalogSource. Subscriptions describe which channel of an operator package to subscribe to and whether to perform updates automatically or manually. If set to automatic, the Subscription ensures OLM will manage and upgrade the operator to ensure the latest version is always running in the cluster.
# * **ClusterServiceVersion (CSV)**:
#     * The metadata that accompanies your Operator container image. It can be used to populate user interfaces with info like your logo/description/version and it is also a source of technical information needed to run the Operator. It includes RBAC rules and which Custom Resources it manages or depends on. OLM will parse this and do all of the hard work to wire up the correct Roles and Role Bindings, ensuring that the Operator is started (or updated) within the desired namespace and check for various other requirements, all without the end users having to do anything. You can easily build your own CSV with [this handy website](https://operatorhub.io/packages) and read about the [full CSV architecture in more detail](https://github.com/operator-framework/operator-lifecycle-manager/blob/master/doc/design/architecture.md#what-is-a-clusterserviceversion).
# * **PackageManifest**:
#     * An entry in the CatalogSource that associates a package identity with sets of CSVs. Within a package, channels point to a particular CSV. Because CSVs explicitly reference the CSV that they replace, a PackageManifest provides OLM with all of the information that is required to update a CSV to the latest version in a channel (stepping through each intermediate version).
# * **InstallPlan**:
#     * Calculated list of resources to be created in order to automatically install or upgrade a CSV.
# * **OperatorGroup**:
#     * Configures all Operators deployed in the same namespace as the OperatorGroup object to watch for their Custom Resource (CR) in a list of namespaces or cluster-wide.

# Observe these CRDs by running the following command:

oc get crd | grep -E 'catalogsource|subscription|clusterserviceversion|packagemanifest|installplan|operatorgroup'


# OLM is powered by controllers that reside within the `openshift-operator-lifecycle-manager` namespace as three Deployments (catalog-operator, olm-operator, and packageserver):

oc -n openshift-operator-lifecycle-manager get deploy

oc get catalogsources -n openshift-marketplace


# Here is a brief summary of each CatalogSource:

# * **Certified Operators**:
#     * All Certified Operators have passed [Red Hat OpenShift Operator Certification](http://connect.redhat.com/explore/red-hat-openshift-operator-certification), an offering under Red Hat Partner Connect, our technology partner program. In this program, Red Hat partners can certify their Operators for use on Red Hat OpenShift. With OpenShift Certified Operators, customers can benefit from validated, well-integrated, mature and supported Operators from Red Hat or partner ISVs in their hybrid cloud environments.

# To view the Operators included in the **Certified Operators** CatalogSource, run the following:

oc get packagemanifests -l catalog=certified-operators

# * **Community Operators**:
#     * With access to community Operators, customers can try out Operators at a variety of maturity levels. Delivering the OperatorHub community, Operators on OpenShift fosters iterative software development and deployment as Developers get self-service access to popular components like databases, message queues or tracing in a managed-service fashion on the platform. These Operators are maintained by relevant representatives in the [operator-framework/community-operators GitHub repository](https://github.com/operator-framework/community-operators).

# To view the Operators included in the **Community Operators** CatalogSource, run the following:

oc get packagemanifests -l catalog=community-operators

# * **Red Hat Operators**:
#     * These Operators are packaged, shipped, and supported by Red Hat.

# To view the Operators included with the **RedHat Operators** CatalogSource, run the following:

oc get packagemanifests -l catalog=redhat-operators

# * **Red Hat Marketplace**:
#     * Built in partnership by Red Hat and IBM, the Red Hat Marketplace helps organizations deliver enterprise software and improve workload portability. Learn more at [marketplace.redhat.com](https://marketplace.redhat.com).

# To view the Operators included in the **Red Hat Marketplace** CatalogSource, run the following:

oc get packagemanifests -l catalog=redhat-marketplace

# You will then able able to login with admin permissions:

# * **Username:** ``admin``{{copy}}
# * **Password:** ``admin``{{copy}}

# The OLM panel appears in the **Operators** pane:

# OLM on the OpenShift Console
# Let's begin my creating a new project called `myproject`.

oc new-project myproject


# We should first create an  OperatorGroup (https://operator-framework.github.io/olm-book/docs/operator-scoping.html) to ensure Operators installed to this namespace will be capable of watching for Custom Resources within the `myproject` namespace:

cat > argocd-operatorgroup.yaml <<EOF
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: argocd-operatorgroup
  namespace: myproject
spec:
  targetNamespaces:
    - myproject
EOF


# Create the OperatorGroup:

oc create -f argocd-operatorgroup.yaml


# Verify the OperatorGroup has been successfully created:

oc get operatorgroup argocd-operatorgroup 

# Create a Subscription manifest for the ArgoCD Operator (https://github.com/argoproj-labs/argocd-operator). Ensure the `installPlanApproval` is set to `Manual`. This will allow us to review the InstallPlan before choosing to install the Operator:

cat > argocd-subscription.yaml <<EOF
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: argocd-operator
  namespace: myproject 
spec:
  channel: alpha
  installPlanApproval: Manual
  name: argocd-operator
  source: community-operators
  sourceNamespace: openshift-marketplace
EOF


# Create the Subscription:

oc create -f argocd-subscription.yaml


# Verify the Subscription was created:

oc get subscription


# The creation of the subscription will also trigger OLM to automatically generate an InstallPlan:

oc get installplan

ARGOCD_INSTALLPLAN=`oc get installplan -o jsonpath={$.items[0].metadata.name}`
oc get installplan $ARGOCD_INSTALLPLAN -o yaml

# You can get a better view of the InstallPlan by navigating to the ArgoCD Operator in the [Console](https://console-openshift-console-[[HOST_SUBDOMAIN]]-443-[[KATACODA_HOST]].environments.katacoda.com/).

# Navigate to the Operators section of the UI and select the ArgoCD Operator under **Installed Operators**. Ensure you are scoped to the **myproject** namespace. You should click on the InstallPlan on the bottom right of the screen:

# InstallPlan on the OpenShift Console

# You can install the Operator by approving the InstallPlan via the OpenShift console or with the following command:

oc patch installplan $ARGOCD_INSTALLPLAN --type='json' -p '[{"op": "replace", "path": "/spec/approved", "value":true}]'


# Once the InstallPlan is approved, you will see the newly provisioned ClusterServiceVersion, ClusterResourceDefinition, Role and RoleBindings, Service Accounts, and Argo-CD Operator Deployment.

oc get clusterserviceversion

oc get crd | grep argoproj.io

oc get sa | grep argocd

oc get roles | grep argocd

oc get rolebindings | grep argocd

oc get deployments


# When the ArgoCD Operator is running, we can observe its logs by running the following:

ARGOCD_OPERATOR=`oc get pods -o jsonpath={$.items[0].metadata.name}`
oc logs $ARGOCD_OPERATOR

# The ArgoCD Operator is now waiting for the creation of an ArgoCD Custom Resource within the `myproject` namespace.Let's deploy our ArgoCD Server "operand" by creating the ArgoCD manifest via the CLI. You can also do this on the OpenShift console.

cat > argocd-cr.yaml <<EOF
apiVersion: argoproj.io/v1alpha1
kind: ArgoCD
metadata:
  name: example-argocd
  namespace: myproject
spec:
  dex:
    image: quay.io/ablock/dex
    openShiftOAuth: true
    version: openshift-connector
  rbac:
    policy: |
      g, system:cluster-admins, role:admin
  server:
    route:
      enabled: true
EOF


# Create the ArgoCD Custom Resource:

oc create -f argocd-cr.yaml


# The ArgoCD Operator should now begin to generate the ArgoCD Operand artifacts. This can take up to one minute:

oc get deployments
oc get secrets
oc get services
oc get routes

ARGOCD_ROUTE=`oc get routes example-argocd-server -o jsonpath={$.spec.host}`
echo $ARGOCD_ROUTE

# Select **Login via OpenShift** to use OpenShift as our identity provider.

# For more information on getting started with ArgoCD on OpenShift 4, check out this video (https://www.youtube.com/watch?v=xYCX2EejSMc).

#You can easily uninstall your operator by first removing the ArgoCD Custom Resource:

oc delete argocd example-argocd

# Removing the ArgoCD Custom Resource, should remove all of the Operator's Operands:

oc get deployments

# And then uninstalling the Operator:

oc delete -f argocd-subscription.yaml
ARGOCD_CSV=`oc get csv -o jsonpath={$.items[0].metadata.name}`
oc delete csv $ARGOCD_CSV


# Once the Subscription and ClusterServiceVersion have been removed, the Operator and associated artifacts will be removed from the cluster:

oc get pods
oc get roles

# ## OLM
#  * Operator Lifecycle Manager: https://github.com/operator-framework/operator-lifecycle-manager/
#  * OLM-Book: https://operator-framework.github.io/olm-book/
#  * Operator SDK Integration with Operator Lifecycle Manager: https://sdk.operatorframework.io/docs/olm-integration/

# ## Chat
# * Kubernetes Slack Chat (upstream): #kubernetes-operators at https://kubernetes.slack.com/
# * Operator-Framework on Google Groups: https://groups.google.com/forum/#!forum/operator-framework
# * OpenShift Operators Special Interest Group (SIG): https://commons.openshift.org/sig/OpenshiftOperators.html
